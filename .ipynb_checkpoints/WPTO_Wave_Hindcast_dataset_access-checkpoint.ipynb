{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOE WPTO Wave Hindcast Data Access\n",
    "\n",
    "This notebook provides examples on accessing the 32-year Wave Hindcast dataset with brief examples on using the data within MHKiT and the SAMs tool. This data was generated via a collaboration between Pacific Northwest National Lab, Sandia National Lab, and the National Renewable Energy Lab and is hosted from Amazon Web Services using the HDF Group's Highly Scalable Data Service (HSDS) for public access.\n",
    "\n",
    "Currently the dataset only includes the U.S. West Coast, but it will grow incrementally over the next few years to include all U.S. regions by 2022. The dataset will also be extended to span 1979-2020 (42 years) by late 2022.\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "There are two major sub-datasets within the overarching dataset: \n",
    "1. The high-spatial-resolution dataset (hereafter the 'spatial dataset/data') spans the U.S. Exclusive Economic Zone (EEZ) with an unstructured grid that has ~200 m resolution in shallow water. The time step resolution for this spatial data is 3-hours and spans 32 years, 01/01/1979  - 12/31/2010.\n",
    " - The spatial dataset is organized into yearly files (each file contains all variables and locations in that year) and they are located on AWS at: `/nrel/US_wave/US_wave_{year}.h5`\n",
    " - This dataset includes the following variables (indexed by 'coordinates' (latitude and longitude), and a 'time_index'):\n",
    "  - directionality_coefficient (no units)\n",
    "  - energy_period (seconds)\n",
    "  - maximum_energy_direction (degrees true, direction from-which waves are travelling)\n",
    "  - mean_absolute_period (seconds)\n",
    "  - mean_zero-crossing_period (seconds)\n",
    "  - omni-directional_wave_power (Watts)\n",
    "  - peak_period (seconds)\n",
    "  - significant_wave_height (meters)\n",
    "  - spectral_width (no units)\n",
    "  - water_depth (meters)\n",
    "  \n",
    "2. The 'virtual buoy dataset' is also available at specific locations within the large spatial domain. These virtual buoys span the same 32-years of the spatial dataset however the time resolution is 1-hour, and these data also include the wave directional spectrum.\n",
    " - The virtual buoy dataset is located on AWS at: `/nrel/US_wave/virtual_buoy/US_virtual_buoy_{year}.h5`\n",
    " - This dataset includes all of the variables listed above, and also includes the wave spectrum (indexed by 'time_index','frequency', 'direction', and 'coordinates'):\n",
    "  - direction_wave_spectrum (meters<sup>2</sup>/(degree*Hz)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and Configuring the HSDS service\n",
    "\n",
    "For this example to work, the packages necessary can be installed via pip:\n",
    "```\n",
    "$ pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Next you'll need to configure h5pyd to access the data on HSDS:\n",
    "```\n",
    "$ hsconfigure\n",
    "```\n",
    "and enter at the prompt:\n",
    "```\n",
    "hs_endpoint = https://developer.nrel.gov/api/hsds   \n",
    "hs_username = None   \n",
    "hs_password = None   \n",
    "hs_api_key = 3K3JQbjZmWctY0xmIfSYvYgtIcM3CN0cb1Y2w9bf    \n",
    "```\n",
    "The example API key here is for demonstation and is rate-limited per IP. To get your own API key, visit https://developer.nrel.gov/signup/\n",
    "\n",
    "The example API key here is for demonstation and is rate-limited per IP. To get your own API key, visit https://developer.nrel.gov/signup/. Alternatively, you can add the above contents to the hsds configuration file: `~/.hscfg`\n",
    "\n",
    "Use [Jupyter Notebook or Jupyter Lab](https://jupyter.org) to view and modify this example notebook. For example, after following the steps above, start a jupyter notebook by:\n",
    "```\n",
    "$ jupyter lab\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage of the Spatial Datasets\n",
    "\n",
    "The following examples illustrate basic examples using NREL-rex to access and download parts of the WPTO Wave Hindcast dataset.\n",
    "\n",
    "Datasets are returned as Pandas objects. See the Pandas [documentation](https://pandas.pydata.org/pandas-docs/stable/) \n",
    "for more information about working with Pandas objects.  \n",
    "\n",
    "We start by viewing the 'index variables' of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick View of metadata, time_index, and coordinates \n",
    "from rex import WaveX\n",
    "\n",
    "waveFile = f'/nrel/US_wave/US_wave_1990.h5'\n",
    "\n",
    "with WaveX(waveFile, hsds=True) as waves:\n",
    "    meta = waves.meta          ## meta is an object that contains location information for each data point\n",
    "    print(\"meta =\", meta)\n",
    "    time_index = waves.time_index # time_index contains all of the years within the file\n",
    "    print(\"time_index =\",time_index)\n",
    "    coordinates = waves.coordinates # coordinates contains all the lat/lon pars within the dataset\n",
    "    print(\"coordinates =\",coordinates)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting data from a single site:\n",
    "A single latitude/longitude pair can be given to extract data nearest to that location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the timeseries of significant_wave_height closest to a coordinate pair\n",
    "from rex import WaveX\n",
    "\n",
    "with WaveX(waveFile, hsds=True) as waves:\n",
    "    lat_lon = (44.624076,-124.280097)\n",
    "    parameters = 'significant_wave_height'\n",
    "    swh_single = waves.get_lat_lon_df(parameters, lat_lon)\n",
    "swh_single"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting data from multiple sites:\n",
    "A list of latitude/longitude pairs can be passed to extract data from multiple sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract the timeseries of significant_wave_height closest to multiple coordinate pairs\n",
    "from rex import WaveX\n",
    "\n",
    "with WaveX(waveFile, hsds=True) as waves:\n",
    "    lat_lon = [(44.624076,-124.280097),(43.489171,-125.152137)] # set lat_lon to a list of lat/lon pairs\n",
    "    parameters = 'significant_wave_height'\n",
    "    swh_multi = waves.get_lat_lon_df(parameters, lat_lon)\n",
    "swh_multi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Data over Multiple Years:\n",
    "The examples above return data for a single year only. To extract data over a number of years, use the `MultiYearWaveX` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatonate yearly significant wave height datasets into a single timeseries\n",
    "from rex import MultiYearWaveX # Yearly concatonation requires the MultiYearResource function\n",
    "\n",
    "multi_year_waves = f'/nrel/US_wave/US_wave_199*.h5' # file names can now accept Wildcards to specify which files to load, lists of filenames with specific years are also supported\n",
    "\n",
    "with MultiYearWaveX(multi_year_waves,hsds=True) as mYears:\n",
    "    lat_lon = (44.624076,-124.280097)\n",
    "    parameters = 'significant_wave_height'\n",
    "    swh_multi_year = mYears.get_lat_lon_df(parameters, lat_lon)\n",
    "    \n",
    "swh_multi_year\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This functionality extends to multiple locations and regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatonate yearly significant wave height datasets for multiple llocations into a single timeseries\n",
    "from rex import MultiYearWaveX \n",
    "\n",
    "multi_year_waves = f'/nrel/US_wave/US_wave_199*.h5' \n",
    "\n",
    "with MultiYearWaveX(multi_year_waves, hsds=True) as waves:\n",
    "    lat_lon = [(44.624076,-124.280097),(43.489171,-125.152137)] # set lat_lon to a list of lat/lon pairs\n",
    "    parameters = 'significant_wave_height'\n",
    "    swh_multi = waves.get_lat_lon_df(parameters, lat_lon)\n",
    "\n",
    "swh_multi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage of the Virtual Buoys\n",
    "\n",
    "Virtual buoys were created during the larger UnSWAN model runs and contain 1-hour timestep data with directional spectrum data\n",
    "\n",
    "These data can be accessed using the same basic approaches described above, but using a different path that points to these data: `/nrel/US_wave/virtual_buoy/US_virtual_buoy_{year}.h5`\n",
    "\n",
    "Two examples are below, the first accessing a single year of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the metadata and time_index for the virtual buoy datasets\n",
    "from rex import WaveX\n",
    "\n",
    "vBuoy = f'/nrel/US_wave/virtual_buoy/US_virtual_buoy_1995.h5'\n",
    "\n",
    "with WaveX(vBuoy, hsds=True) as waves:\n",
    "    meta = waves.meta          ## meta is an object that contains location information for each data point\n",
    "    print(\"meta =\", meta)\n",
    "    time_index = waves.time_index # time_index contains all of the years within the file\n",
    "    print(\"time_index =\",time_index)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second uses `MultiYearWaveX` to get multiple years of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and concatonate multiple years of significant wave height data from teh virtual buoy dataset\n",
    "from rex import MultiYearWaveX\n",
    "\n",
    "multi_year_vBuoy = f'/nrel/US_wave/virtual_buoy/US_virtual_buoy_199*.h5'\n",
    "\n",
    "with MultiYearWaveX(multi_year_vBuoy,hsds=True) as mYears:\n",
    "    lat_lon = (44.624076,-124.280097)\n",
    "    parameters = 'significant_wave_height'\n",
    "    swh_buoy = mYears.get_lat_lon_df(parameters, lat_lon)\n",
    "    \n",
    "swh_buoy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with MHKiT\n",
    "\n",
    "Functionality to read the WPTO hindcast data is currently being integrated into [MHKiT](https://mhkit-software.github.io/MHKiT/), and will be avaiable to users soon. The following examples show how you will be able to use MHKiT to access the dataset once the functions are integrated into MHKiT. In addition to the Python functions demonstrated below, MHKiT-MATLAB functions will also be available with the same functionality.  \n",
    "\n",
    "Note: The following examples currently will NOT run on your local machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing local development version of MHKiT. This code will NOT work on local machines. \n",
    "import sys\n",
    "sys.path.insert(1, '/mnt/c/Users/abharath/Documents/Projects/MHKit/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting data from a single site:\n",
    "A single lat/lon pair can be given to extract data nearest to that location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the timeseries of significant_wave_height closest to a coordinate pair\n",
    "from mhkit.wave import io\n",
    "\n",
    "single_year_waves = f'/nrel/US_wave/US_wave_1995.h5'\n",
    "lat_lon = (44.624076,-124.280097)\n",
    "parameters = 'significant_wave_height'\n",
    "\n",
    "wave_singleYear = io.read_US_wave_dataset(single_year_waves,parameters,lat_lon)\n",
    "\n",
    "wave_singleYear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting data from multiple sites:\n",
    "A list of latitude/longitude pairs can be passed to extract data from multiple sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the timeseries of significant_wave_height closest to multiple coordinate pairs\n",
    "from mhkit.wave import io\n",
    "\n",
    "single_year_waves = f'/nrel/US_wave/US_wave_1995.h5'\n",
    "lat_lon = [(44.624076,-124.280097),(43.489171,-125.152137)]\n",
    "parameters = 'significant_wave_height'\n",
    "\n",
    "wave_multiLocal = io.read_US_wave_dataset(single_year_waves,parameters,lat_lon)\n",
    "\n",
    "wave_multiLocal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Data over Multiple Years:\n",
    "Data can be extracted over a number of years and concatonated directly with the MHKiT tools.\n",
    "The new multi-year object has the same functionality as a single year dataset. The datasets are concatonated into a single timeseries for convenience "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and concatonate a multi-year the timeseries of significant_wave_height closest to a coordinate pair\n",
    "from mhkit.wave import io\n",
    "\n",
    "multi_year_waves = f'/nrel/US_wave/US_wave_199*.h5'\n",
    "lat_lon = (44.624076,-124.280097)\n",
    "parameters = 'significant_wave_height'\n",
    "\n",
    "wave_multiYear = io.read_US_wave_dataset(multi_year_waves,parameters,lat_lon)\n",
    "\n",
    "wave_multiYear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar way to the pervious virtual buoy examples using the rex package, Virtual buoy data can be accessed through MHKiT-Python simply by selecting the correct file location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Dataset Bulk Parameters\n",
    "\n",
    "In the following example we provide a simple means to view the data from the WPTO Wave Hindcast Datasets\n",
    "\n",
    "### Timeseries Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull datasets from AWS\n",
    "from mhkit.wave import io\n",
    "\n",
    "dataset = f'/nrel/US_wave/US_wave_1995.h5'\n",
    "lat_lon = (44.624076,-124.280097)\n",
    "swh_name, owp_name = 'significant_wave_height', 'omni-directional_wave_power'\n",
    "\n",
    "# First we use the MHKiT loading function to grab the datasets from AWS \n",
    "swh = io.read_US_wave_dataset(dataset,swh_name,lat_lon)\n",
    "owp = io.read_US_wave_dataset(dataset,owp_name,lat_lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The timeseries plot example below puts both omni-directional_wave_power and significant wave height on a shared x-axis plot. It is hopefully straight forward to make a change to plot any variable within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format ='retina'\n",
    "%matplotlib widget\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 12}\n",
    "\n",
    "mpl.rc('font', **font)\n",
    "\n",
    "host = host_subplot(111)\n",
    "par = host.twinx()\n",
    "\n",
    "host.set_xlabel(\"Time\")\n",
    "host.set_ylabel('Significant Wave Height (m)')\n",
    "par.set_ylabel('Omni-Directional Wave Power (kW/m)')\n",
    "\n",
    "p1, = host.plot(swh.index, swh.values, label='Significant Wave Height')\n",
    "p2, = par.plot(owp.index, owp.values/1000, label='Omni-Directional Wave Power')\n",
    "\n",
    "leg = plt.legend()\n",
    "\n",
    "host.yaxis.get_label().set_color(p1.get_color())\n",
    "leg.texts[0].set_color(p1.get_color())\n",
    "\n",
    "par.yaxis.get_label().set_color(p2.get_color())\n",
    "leg.texts[1].set_color(p2.get_color())\n",
    "\n",
    "plt.title('WPTO Wave Hindacast Data for 1995')\n",
    "plt.grid(linestyle=':')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing and Working with the Virtual Buoy Direction Wave Spectum\n",
    "\n",
    "Directional wave spectrum data is more complicated to work with in that we have the extra dimentions of frequency and direction within the dataset. These dataset are returned from HSDS as [Pandas Multi-index Dataframes](https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html) and can be analyzed following that syntax.\n",
    "\n",
    "The following examples show the form of the Directional wave spectrum datasets and a method for plotting the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull datasets from AWS (currently being worked into rex)\n",
    "from mhkit.wave import io\n",
    "\n",
    "dataset = f'/nrel/US_wave/virtual_buoy/US_virtual_buoy_1995.h5'\n",
    "lat_lon = (44.624076,-124.280097)\n",
    "spc_name = 'directional_wave_spectrum'\n",
    "\n",
    "# First we use the MHKiT loading function to grab the datasets from AWS \n",
    "spc = io.read_US_wave_dataset(dataset,spc_name,lat_lon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperary representitive dataset saved locally\n",
    "from pandas import read_pickle\n",
    "\n",
    "spc = read_pickle('../multi_index_example.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Directional Wave Spectrum data we can create Polar plots of the data. In what follows we plot the mean of the 1996 spectrum data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format ='retina'\n",
    "%matplotlib widget\n",
    "\n",
    "from numpy import radians, meshgrid, unique, hstack, vstack, array, pi, mean, newaxis\n",
    "from numpy.random import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import matplotlib as mpl\n",
    "\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 10}\n",
    "\n",
    "mpl.rc('font', **font)\n",
    "\n",
    "\n",
    "azimuths = hstack([array(0),hstack([unique(radians(spc.index.get_level_values(level=2))),2*pi])])\n",
    "zeniths = unique(spc.index.get_level_values(level=1))\n",
    "\n",
    "r, theta = meshgrid(zeniths, azimuths)\n",
    "theta = 90-theta\n",
    "\n",
    "values = spc.mean(level=(1,2)).values.reshape(zeniths.shape[0],azimuths.shape[0]-2)\n",
    "values[values<=0] = 1e-9\n",
    "\n",
    "edges = mean([values[:,0],values[:,-1]],axis=0)\n",
    "values = hstack([edges[:,newaxis],hstack([values,edges[:,newaxis]])])\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw=dict(projection='polar'))\n",
    "cs = ax.contourf(theta, r, values.T, locator=ticker.LogLocator())\n",
    "ax.set_theta_zero_location(\"N\")\n",
    "ax.set_xticklabels(['N', 'NW', 'W', 'SW', 'S', 'SE', 'E', 'NE'])\n",
    "\n",
    "label_position=ax.get_rlabel_position()\n",
    "ax.text(radians(label_position+10),ax.get_rmax()/2.,'Frequency (Hz)',\n",
    "        rotation=label_position-90,ha='center',va='center')\n",
    "ax.set_title(\"Directional Wave Spectrum\", va='bottom')\n",
    "\n",
    "cbar = plt.colorbar(cs)\n",
    "cbar.ax.set_ylabel(r'Frequency Amplitude (m^2/Hz/deg)', rotation=270,fontsize=font['size'],fontweight=font['weight'])\n",
    "plt.tight_layout()\n",
    "plt.grid(linestyle=':',color='k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example calculates a JPD between Hm0 and Te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mhkit.wave import io\n",
    "\n",
    "dataset = f'/nrel/US_wave/virtual_buoy/US_virtual_buoy_1995.h5'\n",
    "lat_lon = (44.624076,-124.280097)\n",
    "Hm0_name, Te_name = 'significant_wave_height', 'energy_period'\n",
    "\n",
    "# First we use the MHKiT loading function to grab the datasets from AWS \n",
    "Hm0 = io.read_US_wave_dataset(dataset,Hm0_name,lat_lon)\n",
    "Te = io.read_US_wave_dataset(dataset,Te_name,lat_lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from numpy import arange, zeros, int32, mean, array, sum\n",
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "# Generate bins for Hm0 and Te, input format (start, stop, step_size)\n",
    "Hm0_bins = arange(0, Hm0.values.max() + 0.5, 0.5)    \n",
    "Te_bins = arange(0, Te.values.max() + 1, 1)\n",
    "\n",
    "Hm0_center = array([mean([Hm0_bins[i+1],Hm0_bins[i]]) for i in range(Hm0_bins.shape[0]-1)])\n",
    "Te_center = array([mean([Te_bins[i+1],Te_bins[i]]) for i in range(Te_bins.shape[0]-1)])\n",
    "\n",
    "ret = stats.binned_statistic_2d(Hm0.values, Te.values, None, 'count', bins=[Hm0_bins,Te_bins],expand_binnumbers=True)\n",
    "\n",
    "JPD_matrix = zeros([Hm0_center.shape[0],Te_center.shape[0]],dtype=int32)\n",
    "\n",
    "for i in range(ret.binnumber.shape[-1]): \n",
    "    H_idx, T_idx = ret.binnumber[:,i]\n",
    "    JPD_matrix[H_idx-1,T_idx-1] = JPD_matrix[H_idx-1,T_idx-1] + 1 \n",
    "\n",
    "JPD_probability = 2*(JPD_matrix/Hm0.index.shape[0])\n",
    "\n",
    "JPD_probability = DataFrame(JPD_probability, index=Hm0_center, columns=Te_center)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format ='retina'\n",
    "%matplotlib widget\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 10}\n",
    "\n",
    "mpl.rc('font', **font)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mhkit.wave.graphics import plot_matrix\n",
    "\n",
    "# Plot the Plot mean matrix\n",
    "\n",
    "ax = plot_matrix(JPD_probability,xlabel='Te (s)',ylabel='Hm0 (m)',zlabel='Sea-State Expectation (1/(sec*m))', show_values=False)\n",
    "plt.title('Joint Probability Distribution')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
